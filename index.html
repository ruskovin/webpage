<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mod√®les & M√©triques - Sales Forecasting</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            color: #2c3e50;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        
        header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 15px;
            font-weight: 700;
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.95;
            max-width: 800px;
            margin: 0 auto;
        }
        
        nav {
            background: #34495e;
            padding: 0;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
        }
        
        nav li {
            margin: 0;
        }
        
        nav a {
            display: block;
            color: white;
            text-decoration: none;
            padding: 18px 25px;
            transition: background 0.3s;
            font-weight: 500;
        }
        
        nav a:hover {
            background: #2c3e50;
        }
        
        .content {
            padding: 50px 60px;
        }
        
        section {
            margin-bottom: 60px;
        }
        
        h2 {
            color: #1e3c72;
            font-size: 2em;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 3px solid #3498db;
        }
        
        h3 {
            color: #2a5298;
            font-size: 1.5em;
            margin: 30px 0 15px 0;
        }
        
        h4 {
            color: #34495e;
            font-size: 1.2em;
            margin: 20px 0 10px 0;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        .model-box {
            background: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 25px;
            margin: 25px 0;
            border-radius: 4px;
        }
        
        .formula {
            background: #ecf0f1;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            text-align: center;
            border: 2px solid #bdc3c7;
        }
        
        .highlight {
            background: #e3f2fd;
            padding: 20px;
            border-left: 4px solid #2196f3;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            background: white;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        th {
            background: #2a5298;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        
        td {
            padding: 12px 15px;
            border-bottom: 1px solid #ddd;
        }
        
        tr:hover {
            background: #f5f5f5;
        }
        
        .advantage {
            color: #27ae60;
            font-weight: 600;
        }
        
        .limitation {
            color: #e74c3c;
            font-weight: 600;
        }
        
        footer {
            background: #2c3e50;
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        .references {
            background: #f8f9fa;
            padding: 30px;
            margin: 30px 0;
            border-radius: 5px;
        }
        
        .references ol {
            margin-left: 20px;
        }
        
        .references li {
            margin-bottom: 15px;
            line-height: 1.6;
        }
        
        .key-point {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 15px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Mod√®les & M√©triques d'√âvaluation</h1>
            <p>Note Technique ‚Äì Pr√©vision des Ventes Retail<br>Inspir√© du Kaggle Corporaci√≥n Favorita Grocery Sales Forecasting</p>
        </header>
        
        <nav>
            <ul>
                <li><a href="#intro">Introduction</a></li>
                <li><a href="#base-finale">Base Finale</a></li>
                <li><a href="#modeles">Mod√®les</a></li>
                <li><a href="#metriques">M√©triques</a></li>
                <li><a href="#bonnes-pratiques">Bonnes Pratiques</a></li>
                <li><a href="#references">R√©f√©rences</a></li>
            </ul>
        </nav>
        
        <div class="content">
            <section id="intro">
                <h2>1. Introduction G√©n√©rale</h2>
                
                <p>La pr√©vision des ventes constitue un enjeu strat√©gique majeur dans le secteur du retail. Elle permet d'optimiser la gestion des stocks, de minimiser les ruptures et les surstocks, et d'am√©liorer la planification op√©rationnelle. Dans un contexte de grande distribution, o√π des milliers de produits sont vendus quotidiennement dans de multiples points de vente, la capacit√© √† anticiper pr√©cis√©ment la demande devient un avantage comp√©titif d√©terminant.</p>
                
                <p>Le probl√®me de pr√©vision des ventes s'inscrit dans le domaine des s√©ries temporelles et n√©cessite de traiter des donn√©es transactionnelles caract√©ris√©es par une forte saisonnalit√©, des tendances temporelles, et des comportements non-lin√©aires complexes. Les donn√©es retail pr√©sentent √©galement des d√©fis sp√©cifiques : distributions asym√©triques, pr√©sence de nombreux z√©ros (produits non vendus certains jours), et h√©t√©rog√©n√©it√© importante entre les produits et les magasins.</p>
                
                <div class="highlight">
                    <h4>Importance de la comparaison de mod√®les</h4>
                    <p>La comparaison rigoureuse de plusieurs familles de mod√®les constitue une √©tape centrale en data science. Elle permet de :</p>
                    <p><strong>‚Ä¢ √âtablir un baseline de r√©f√©rence</strong> pour quantifier l'apport r√©el du machine learning</p>
                    <p><strong>‚Ä¢ Identifier les compromis</strong> entre performance pr√©dictive, complexit√© et temps de calcul</p>
                    <p><strong>‚Ä¢ S√©lectionner le mod√®le le plus adapt√©</strong> au contexte m√©tier et aux contraintes de production</p>
                    <p><strong>‚Ä¢ Justifier scientifiquement</strong> les choix techniques aupr√®s des parties prenantes</p>
                </div>
                
                <p>Les comp√©titions Kaggle, et notamment la comp√©tition Corporaci√≥n Favorita Grocery Sales Forecasting, offrent un cadre standardis√© permettant de comparer objectivement diff√©rentes approches m√©thodologiques. Ces comp√©titions refl√®tent des probl√©matiques industrielles r√©elles et ont contribu√© √† √©tablir les meilleures pratiques en mati√®re de forecasting retail. Les solutions gagnantes combinent g√©n√©ralement feature engineering sophistiqu√©, ensembles de mod√®les, et m√©triques d'√©valuation adapt√©es aux sp√©cificit√©s du domaine.</p>
            </section>
            <section id="base-finale">
    <h2>2. Pr√©sentation de la Base de Donn√©es Finale</h2>

    <p>
        La performance d‚Äôun mod√®le de pr√©vision des ventes d√©pend autant du choix de l‚Äôalgorithme
        que de la qualit√© de la base de donn√©es utilis√©e. Une attention particuli√®re a donc √©t√© port√©e
        au <strong>feature engineering</strong>, √† la transformation de la variable cible et √† l‚Äôint√©gration
        de variables exog√®nes pertinentes (calendrier, √©conomie, √©v√©nements).
    </p>

    <div class="highlight">
        <h4>Objectif de la construction de la base</h4>
        <p>
            Construire une base <strong>riche, stable et √©conomiquement interpr√©table</strong>, capable
            de capturer √† la fois :
        </p>
        <p>
            ‚Ä¢ les dynamiques temporelles des ventes<br>
            ‚Ä¢ les effets retard√©s (lags)<br>
            ‚Ä¢ les chocs exog√®nes (p√©trole, jours f√©ri√©s, promotions)<br>
            ‚Ä¢ l‚Äôh√©t√©rog√©n√©it√© produits / magasins
        </p>
    </div>

    <!-- ================= TARGET ================= -->
    <h3>2.1 üéØ Variable Cible (Target)</h3>

    <div class="model-box">
        <h4><strong>unit_sales_win</strong> ‚Äî Nombre d‚Äôunit√©s vendues</h4>

        <p>
            La variable cible repr√©sente le nombre d‚Äôunit√©s vendues par
            <strong>produit‚Äìmagasin‚Äìjour</strong>.
            √âtant fortement asym√©trique et sujette √† des valeurs extr√™mes,
            elle a subi deux transformations successives.
        </p>

        <h4>1Ô∏è‚É£ Winsorisation</h4>
        <p>
            Les valeurs extr√™mes ont √©t√© plafonn√©es entre le
            <strong>5<sup>e</sup> et le 99.5<sup>e</sup> percentile</strong>.
            Cette op√©ration permet de :
        </p>
        <p>
            ‚Ä¢ r√©duire l‚Äôinfluence disproportionn√©e des outliers<br>
            ‚Ä¢ stabiliser l‚Äôapprentissage des mod√®les<br>
            ‚Ä¢ pr√©server l‚Äôinformation globale sans supprimer d‚Äôobservations
        </p>

        <h4>2Ô∏è‚É£ Transformation Logarithmique</h4>
        <div class="formula">
            y = log(1 + unit_sales_win)
        </div>

        <p>
            La transformation <strong>log1p</strong> r√©duit l‚Äôasym√©trie de la distribution,
            stabilise la variance et rend la cible plus compatible avec les hypoth√®ses
            des mod√®les (en particulier lin√©aires et boosting).
        </p>
    </div>

    <!-- ================= FEATURES ================= -->
    <h3>2.2 üìä Variables Explicatives (Features)</h3>

    <!-- TEMPORAL -->
    <div class="model-box">
        <h4>üïí Variables Temporelles</h4>
        <p>
            Issues directement de la date, ces variables permettent de capturer
            la saisonnalit√© et les effets calendaires.
        </p>

        <p>
            ‚Ä¢ <strong>day</strong> : jour du mois (1‚Äì31)<br>
            ‚Ä¢ <strong>month</strong> : mois de l‚Äôann√©e (1‚Äì12)<br>
            ‚Ä¢ <strong>year</strong> : ann√©e<br>
            ‚Ä¢ <strong>day_of_week</strong> : jour de la semaine (1 = lundi, 7 = dimanche)<br>
            ‚Ä¢ <strong>is_weekend</strong> : indicateur week-end<br>
            ‚Ä¢ <strong>is_payday</strong> : jour de paie (15 ou fin de mois, effet consommation)<br>
            ‚Ä¢ <strong>is_year_end</strong> : p√©riode de fin d‚Äôann√©e (apr√®s le 20 d√©cembre)
        </p>
    </div>

    <!-- OIL -->
    <div class="model-box">
        <h4>üõ¢Ô∏è Variables √âconomiques ‚Äì Prix du P√©trole</h4>
        <p>
            L‚Äô√©conomie √©quatorienne √©tant fortement d√©pendante du p√©trole,
            son int√©gration permet de capturer des effets macro√©conomiques indirects
            sur la consommation.
        </p>

        <p>
            ‚Ä¢ <strong>dcoilwtico</strong> : prix journalier du p√©trole brut (WTI)<br>
            ‚Ä¢ <strong>oil_smooth_7d</strong> : moyenne mobile 7 jours (lissage)<br>
            ‚Ä¢ <strong>oil_lag_10</strong> : prix du p√©trole avec un retard de 10 jours
        </p>
    </div>

    <!-- EVENTS -->
    <div class="model-box">
        <h4>üéâ Vacances et √âv√©nements</h4>
        <p>
            Les jours f√©ri√©s et √©v√©nements ont un impact direct sur les comportements d‚Äôachat.
            Leur effet d√©pend de leur port√©e g√©ographique.
        </p>

        <p>
            ‚Ä¢ <strong>n_events_total</strong> : nombre total d‚Äô√©v√©nements<br>
            ‚Ä¢ <strong>is_holiday_event</strong> : indicateur binaire d‚Äô√©v√©nement<br>
            ‚Ä¢ <strong>is_national_holiday</strong> : jour f√©ri√© national<br>
            ‚Ä¢ <strong>n_nat / n_reg / n_loc</strong> : √©v√©nements nationaux, r√©gionaux, locaux<br>
            ‚Ä¢ <strong>n_states_affected</strong> / <strong>n_cities_affected</strong>
        </p>
    </div>

    <!-- LAGS -->
    <div class="model-box">
        <h4>üìà Historique des Ventes (Lags & Fen√™tres)</h4>
        <p>
            Ces variables capturent la d√©pendance temporelle des ventes,
            essentielle en pr√©vision de s√©ries temporelles.
        </p>

        <p>
            ‚Ä¢ <strong>sales_lag_16 / 21 / 28</strong> : ventes pass√©es (effet m√©moire)<br>
            ‚Ä¢ <strong>sales_roll_mean_7 / 28</strong> : tendance r√©cente<br>
            ‚Ä¢ <strong>sales_roll_std_7</strong> : volatilit√© des ventes
        </p>
    </div>

    <!-- TARGET ENCODING -->
    <div class="model-box">
        <h4>üè∑Ô∏è Encodage des Variables Cat√©gorielles (Target Encoding)</h4>
        <p>
            Les variables cat√©gorielles sont remplac√©es par leur
            <strong>niveau moyen historique de ventes</strong>,
            permettant une repr√©sentation num√©rique compacte et informative.
        </p>

        <p>
            ‚Ä¢ <strong>store_nbr_target_enc</strong><br>
            ‚Ä¢ <strong>item_nbr_target_enc</strong><br>
            ‚Ä¢ <strong>family_target_enc</strong><br>
            ‚Ä¢ <strong>city_target_enc / cluster_target_enc / type_target_enc</strong>
        </p>
    </div>

    <!-- PRODUCT INFO -->
    <div class="model-box">
        <h4>üè™ Informations Produits & Magasins</h4>
        <p>
            ‚Ä¢ <strong>onpromotion</strong> : indicateur de promotion<br>
            ‚Ä¢ <strong>perishable</strong> : produit p√©rissable (pond√©ration plus forte)<br>
            ‚Ä¢ <strong>class</strong> : classe num√©rique du produit
        </p>
    </div>

    <div class="key-point">
        <h4>R√©sultat Final</h4>
        <p>
            La base finale combine <strong>signaux temporels, √©conomiques et comportementaux</strong>,
            offrant un compromis optimal entre richesse informationnelle,
            stabilit√© statistique et performance pr√©dictive.
        </p>
    </div>
</section>

            <section id="modeles">
                <h2>3. Mod√®les Test√©s</h2>
                
                <h3>3.1 Baseline : Moyenne Mobile</h3>
                <div class="model-box">
                    <h4>Principe th√©orique</h4>
                    <p>La moyenne mobile √† 7 jours calcule la pr√©vision comme la moyenne arithm√©tique des ventes observ√©es sur les 7 derniers jours. Pour un jour <em>t</em>, la pr√©vision est donn√©e par :</p>
                    
                    <div class="formula">
                        ≈∑<sub>t</sub> = (1/7) √ó Œ£<sub>i=1</sub><sup>7</sup> y<sub>t-i</sub>
                    </div>
                    
                    <h4>R√¥le fondamental du baseline</h4>
                    <p>Le baseline n'est pas un simple point de d√©part : il constitue une r√©f√©rence critique qui permet de mesurer objectivement la valeur ajout√©e des mod√®les de machine learning. Un mod√®le complexe qui ne surpasse pas significativement un baseline simple ne justifie pas son d√©ploiement en production. Dans le contexte retail, la moyenne mobile capture naturellement la tendance r√©cente et constitue une pr√©vision raisonnable en l'absence d'√©v√©nements exceptionnels.</p>
                    
                    <h4><span class="advantage">Avantages</span></h4>
                    <p>‚Ä¢ Simplicit√© d'impl√©mentation et d'interpr√©tation<br>
                    ‚Ä¢ Aucun entra√Ænement n√©cessaire<br>
                    ‚Ä¢ Robustesse aux outliers temporaires<br>
                    ‚Ä¢ Latence minimale en production</p>
                    
                    <h4><span class="limitation">Limites</span></h4>
                    <p>‚Ä¢ Incapacit√© √† capturer les tendances de long terme<br>
                    ‚Ä¢ Absence de prise en compte de la saisonnalit√© hebdomadaire ou mensuelle<br>
                    ‚Ä¢ Pas d'int√©gration de features exog√®nes (promotions, jours f√©ri√©s)<br>
                    ‚Ä¢ Performance d√©grad√©e lors d'√©v√©nements exceptionnels</p>
                </div>
                
                <h3>3.2 Mod√®les Lin√©aires</h3>
                <div class="model-box">
                    <h4>Principe th√©orique g√©n√©ral</h4>
                    <p>Les mod√®les lin√©aires supposent une relation lin√©aire entre les features <strong>X</strong> et la variable cible <strong>y</strong>. Le mod√®le s'exprime sous la forme :</p>
                    
                    <div class="formula">
                        ≈∑ = Œ≤<sub>0</sub> + Œ≤<sub>1</sub>x<sub>1</sub> + Œ≤<sub>2</sub>x<sub>2</sub> + ... + Œ≤<sub>p</sub>x<sub>p</sub> + Œµ
                    </div>
                    
                    <p>o√π les coefficients Œ≤ sont estim√©s en minimisant une fonction de perte, g√©n√©ralement l'erreur quadratique moyenne (MSE).</p>
                    
                    <h4>R√©gression Lin√©aire (Ordinary Least Squares)</h4>
                    <p>La r√©gression lin√©aire classique minimise directement la somme des carr√©s des r√©sidus sans contrainte sur les coefficients. Elle fournit une solution analytique mais est sensible √† la multicolin√©arit√© et au sur-apprentissage lorsque le nombre de features est √©lev√©.</p>
                    
                    <h4>Ridge Regression (R√©gularisation L2)</h4>
                    <p>Ridge ajoute une p√©nalit√© L2 sur la norme des coefficients √† la fonction de perte :</p>
                    
                    <div class="formula">
                        Loss = MSE + Œª √ó Œ£ Œ≤<sub>j</sub><sup>2</sup>
                    </div>
                    
                    <p>Le param√®tre Œª contr√¥le l'intensit√© de la r√©gularisation. Ridge r√©duit la variance du mod√®le en "r√©tr√©cissant" les coefficients vers z√©ro sans les annuler compl√®tement, ce qui am√©liore la g√©n√©ralisation sur des donn√©es non vues.</p>
                    
                    <h4>Lasso Regression (R√©gularisation L1)</h4>
                    <p>Lasso utilise une p√©nalit√© L1 :</p>
                    
                    <div class="formula">
                        Loss = MSE + Œª √ó Œ£ |Œ≤<sub>j</sub>|
                    </div>
                    
                    <p>Contrairement √† Ridge, Lasso peut annuler compl√®tement certains coefficients, effectuant ainsi une s√©lection automatique de features. Cette propri√©t√© est particuli√®rement utile lorsque de nombreuses features sont faiblement corr√©l√©es √† la cible.</p>
                    
                    <h4>ElasticNet</h4>
                    <p>ElasticNet combine les p√©nalit√©s L1 et L2 :</p>
                    
                    <div class="formula">
                        Loss = MSE + Œª<sub>1</sub> √ó Œ£ |Œ≤<sub>j</sub>| + Œª<sub>2</sub> √ó Œ£ Œ≤<sub>j</sub><sup>2</sup>
                    </div>
                    
                    <p>Cette combinaison permet de b√©n√©ficier √† la fois de la s√©lection de features de Lasso et de la stabilit√© de Ridge, particuli√®rement efficace en pr√©sence de groupes de features corr√©l√©es.</p>
                    
                    <div class="key-point">
                        <h4>Biais-Variance Tradeoff</h4>
                        <p>La r√©gularisation illustre le compromis fondamental entre biais et variance : en ajoutant du biais (via la contrainte sur les coefficients), on r√©duit la variance et donc le risque de sur-apprentissage. Le choix de Œª d√©termine ce compromis et doit √™tre optimis√© par validation crois√©e.</p>
                    </div>
                    
                    <h4><span class="advantage">Avantages pour le forecasting retail</span></h4>
                    <p>‚Ä¢ Interpr√©tabilit√© √©lev√©e des coefficients<br>
                    ‚Ä¢ Entra√Ænement rapide m√™me sur de grands datasets<br>
                    ‚Ä¢ Robustesse avec r√©gularisation appropri√©e<br>
                    ‚Ä¢ Gestion efficace de la multicolin√©arit√©</p>
                    
                    <h4><span class="limitation">Limites</span></h4>
                    <p>‚Ä¢ Hypoth√®se de lin√©arit√© souvent viol√©e dans les donn√©es retail<br>
                    ‚Ä¢ Incapacit√© √† capturer les interactions complexes entre features<br>
                    ‚Ä¢ Performance limit√©e face √† des patterns non-lin√©aires (saisonnalit√©, promotions)<br>
                    ‚Ä¢ N√©cessite un feature engineering pouss√© pour compenser les limitations</p>
                </div>
                
                <h3>3.3 Mod√®les Bas√©s sur les Arbres</h3>
                <div class="model-box">
                    <h4>Decision Tree (Arbre de D√©cision)</h4>
                    <p>Un arbre de d√©cision partitionne r√©cursivement l'espace des features en r√©gions homog√®nes. √Ä chaque n≈ìud, l'algorithme s√©lectionne la feature et le seuil qui maximisent une mesure de puret√© (r√©duction de variance pour la r√©gression). La pr√©vision pour une observation correspond √† la moyenne des valeurs cibles dans la feuille terminale atteinte.</p>
                    
                    <p>Bien que les arbres individuels capturent naturellement les non-lin√©arit√©s et les interactions, ils souffrent de forte variance : de petites variations dans les donn√©es d'entra√Ænement peuvent produire des arbres tr√®s diff√©rents. Cette instabilit√© limite leur utilisation isol√©e en production.</p>
                    
                    <h4>Random Forest</h4>
                    <p>Random Forest impl√©mente le principe du <strong>bagging</strong> (Bootstrap Aggregating) appliqu√© aux arbres de d√©cision. L'algorithme construit un ensemble de <em>B</em> arbres sur des √©chantillons bootstrap des donn√©es d'entra√Ænement, en introduisant une randomisation suppl√©mentaire lors du choix des features √† chaque split.</p>
                    
                    <div class="formula">
                        ≈∑ = (1/B) √ó Œ£<sub>b=1</sub><sup>B</sup> T<sub>b</sub>(x)
                    </div>
                    
                    <p>o√π T<sub>b</sub>(x) est la pr√©vision du b-i√®me arbre.</p>
                    
                    <div class="key-point">
                        <h4>R√©duction de la variance par bagging</h4>
                        <p>Le bagging r√©duit la variance sans augmenter le biais en moyennant les pr√©visions de multiples mod√®les faiblement corr√©l√©s. Math√©matiquement, si chaque arbre a une variance œÉ¬≤ et que les arbres sont d√©corr√©l√©s, la variance de l'ensemble est œÉ¬≤/B. En pratique, la corr√©lation r√©siduelle limite cette r√©duction, d'o√π l'importance de la randomisation des features.</p>
                    </div>
                    
                    <h4>Gestion des non-lin√©arit√©s</h4>
                    <p>Les mod√®les √† base d'arbres excellent dans la capture de relations non-lin√©aires complexes et d'interactions d'ordre √©lev√© sans n√©cessiter de feature engineering explicite. Pour le retail, cela permet de mod√©liser naturellement des ph√©nom√®nes tels que l'impact diff√©renci√© des promotions selon le type de produit, le jour de la semaine ou la saison.</p>
                    
                    <h4><span class="advantage">Avantages</span></h4>
                    <p>‚Ä¢ Capture automatique des non-lin√©arit√©s et interactions<br>
                    ‚Ä¢ Robustesse aux outliers et aux features non normalis√©es<br>
                    ‚Ä¢ Peu sensible aux transformations monotones des features<br>
                    ‚Ä¢ Fourniture de mesures d'importance des features</p>
                    
                    <h4><span class="limitation">Limites</span></h4>
                    <p>‚Ä¢ Temps d'entra√Ænement √©lev√© pour de grands ensembles d'arbres<br>
                    ‚Ä¢ Taille importante du mod√®le en m√©moire<br>
                    ‚Ä¢ Interpr√©tabilit√© r√©duite compar√©e aux mod√®les lin√©aires<br>
                    ‚Ä¢ Risque de sur-apprentissage si les arbres ne sont pas suffisamment r√©gularis√©s</p>
                </div>
                
                <h3>3.4 Gradient Boosting : LightGBM</h3>
                <div class="model-box">
                    <h4>Principe du Gradient Boosting</h4>
                    <p>Le Gradient Boosting construit s√©quentiellement un ensemble de mod√®les faibles (g√©n√©ralement des arbres peu profonds) o√π chaque nouveau mod√®le corrige les erreurs des mod√®les pr√©c√©dents. √Ä l'it√©ration <em>m</em>, le mod√®le ajust√© est :</p>
                    
                    <div class="formula">
                        F<sub>m</sub>(x) = F<sub>m-1</sub>(x) + ŒΩ √ó h<sub>m</sub>(x)
                    </div>
                    
                    <p>o√π h<sub>m</sub> est un nouvel arbre entra√Æn√© sur le gradient n√©gatif de la fonction de perte, et ŒΩ est le learning rate qui contr√¥le la contribution de chaque arbre.</p>
                    
                    <h4>Sp√©cificit√©s de LightGBM</h4>
                    <p><strong>Histogram-based learning</strong> : LightGBM discr√©tise les features continues en histogrammes (bins), r√©duisant drastiquement la complexit√© de recherche des meilleurs splits. Cette approche acc√©l√®re l'entra√Ænement de plusieurs ordres de grandeur tout en pr√©servant la qualit√© pr√©dictive.</p>
                    
                    <p><strong>Leaf-wise growth</strong> : Contrairement aux approches level-wise (XGBoost), LightGBM cro√Æt les arbres en choisissant la feuille qui maximise la r√©duction de perte, ind√©pendamment du niveau de profondeur. Cette strat√©gie permet d'obtenir des arbres plus asym√©triques mais souvent plus pr√©cis avec moins de feuilles.</p>
                    
                    <p><strong>Gradient-based One-Side Sampling (GOSS)</strong> : LightGBM √©chantillonne de mani√®re non uniforme les observations, en conservant toutes celles avec de grands gradients (erreurs importantes) et en sous-√©chantillonnant les autres. Cette technique maintient la qualit√© de l'information tout en r√©duisant le co√ªt computationnel.</p>
                    
                    <p><strong>Exclusive Feature Bundling (EFB)</strong> : Les features mutuellement exclusives (qui ne prennent jamais de valeurs non-nulles simultan√©ment) sont regroup√©es pour r√©duire la dimensionnalit√© effective, particuli√®rement efficace avec les encodages one-hot.</p>
                    
                    <div class="highlight">
                        <h4>Pourquoi LightGBM pour le retail ?</h4>
                        <p>Les datasets retail pr√©sentent typiquement des millions d'observations (transactions journali√®res sur de multiples produits et magasins) et des centaines de features (caract√©ristiques produits, magasins, calendrier, lags, statistiques roulantes). LightGBM est sp√©cifiquement con√ßu pour ces situations de haute dimensionnalit√© :</p>
                        <p>‚Ä¢ <strong>Scalabilit√©</strong> : Temps d'entra√Ænement lin√©aire ou sub-lin√©aire par rapport au nombre d'observations<br>
                        ‚Ä¢ <strong>Efficacit√© m√©moire</strong> : Consommation RAM r√©duite gr√¢ce aux histogrammes<br>
                        ‚Ä¢ <strong>Performance</strong> : Obtient r√©guli√®rement les meilleurs scores sur les comp√©titions Kaggle retail<br>
                        ‚Ä¢ <strong>Flexibilit√©</strong> : Support natif de fonctions de perte personnalis√©es et de features cat√©gorielles</p>
                    </div>
                    
                    <h4>Comparaison avec XGBoost et CatBoost</h4>
                    <p><strong>XGBoost</strong> utilise une strat√©gie level-wise et une recherche exacte ou approximative des splits. Bien qu'historiquement le standard du gradient boosting, il est g√©n√©ralement plus lent que LightGBM sur de grands datasets, bien que parfois l√©g√®rement plus pr√©cis sur de petits datasets structur√©s.</p>
                    
                    <p><strong>CatBoost</strong> excelle dans le traitement des features cat√©gorielles via un encodage sophistiqu√© (ordered target statistics). Il offre √©galement une bonne robustesse aux hyperparam√®tres par d√©faut. Cependant, son temps d'entra√Ænement est souvent sup√©rieur √† LightGBM, et ses performances pr√©dictives sont comparables dans la plupart des cas.</p>
                    
                    <p>Pour le forecasting retail √† grande √©chelle, LightGBM repr√©sente g√©n√©ralement le meilleur compromis performance/vitesse, expliquant sa popularit√© dans l'industrie et les comp√©titions.</p>
                    
                    <h4><span class="advantage">Avantages</span></h4>
                    <p>‚Ä¢ Performances pr√©dictives state-of-the-art sur donn√©es tabulaires<br>
                    ‚Ä¢ Temps d'entra√Ænement et d'inf√©rence tr√®s comp√©titifs<br>
                    ‚Ä¢ Gestion native des valeurs manquantes<br>
                    ‚Ä¢ R√©gularisation efficace contre le sur-apprentissage<br>
                    ‚Ä¢ Parall√©lisation optimis√©e (CPU et GPU)</p>
                    
                    <h4><span class="limitation">Limites</span></h4>
                    <p>‚Ä¢ Sensibilit√© aux hyperparam√®tres (n√©cessite tuning soigneux)<br>
                    ‚Ä¢ Risque de sur-apprentissage si mal configur√©<br>
                    ‚Ä¢ Interpr√©tabilit√© globale limit√©e (mod√®les ensemblistes complexes)<br>
                    ‚Ä¢ Peut sous-performer sur de tr√®s petits datasets</p>
                </div>
            </section>
            
            <section id="metriques">
                <h2>4. M√©triques d'√âvaluation</h2>
                
                <h3>4.1 NWRMSLE (Normalized Weighted Root Mean Squared Logarithmic Error)</h3>
                <div class="model-box">
                    <h4>Formule math√©matique</h4>
                    <div class="formula">
                        NWRMSLE = ‚àö[ (Œ£ w<sub>i</sub> √ó (log(≈∑<sub>i</sub> + 1) - log(y<sub>i</sub> + 1))¬≤) / (Œ£ w<sub>i</sub>) ]
                    </div>
                    
                    <p>o√π :<br>
                    ‚Ä¢ y<sub>i</sub> repr√©sente les ventes r√©elles<br>
                    ‚Ä¢ ≈∑<sub>i</sub> repr√©sente les pr√©visions<br>
                    ‚Ä¢ w<sub>i</sub> sont les poids associ√©s √† chaque observation<br>
                    ‚Ä¢ log d√©signe le logarithme naturel</p>
                    
                    <h4>Interpr√©tation √©conomique</h4>
                    <p>La transformation logarithmique rend la m√©trique sensible aux <strong>erreurs relatives</strong> plut√¥t qu'absolues. Une erreur de 10 unit√©s sur une pr√©vision de 20 unit√©s est p√©nalis√©e plus s√©v√®rement qu'une erreur de 10 unit√©s sur une pr√©vision de 1000 unit√©s. Cette propri√©t√© est cruciale en retail o√π :</p>
                    
                    <p>‚Ä¢ Les produits √† faible volume m√©ritent une attention proportionnelle (rupture co√ªteuse m√™me pour peu d'unit√©s)<br>
                    ‚Ä¢ La comparaison entre produits de volumes tr√®s diff√©rents devient √©quitable<br>
                    ‚Ä¢ Les performances sur l'ensemble du catalogue sont mieux √©valu√©es</p>
                    
                    <p>L'ajout de +1 avant le logarithme √©vite les probl√®mes math√©matiques avec les ventes nulles (log(0) non d√©fini) et att√©nue l'impact disproportionn√© des tr√®s petites valeurs.</p>
                    
                    <h4>Usage dans Kaggle Favorita</h4>
                    <p>La comp√©tition Favorita utilisait le NWRMSLE avec des poids w<sub>i</sub> refl√©tant l'importance strat√©gique de certains produits ou magasins. Cette m√©trique encourage les mod√®les √† bien performer sur l'ensemble du portefeuille produits plut√¥t que de se sp√©cialiser sur les best-sellers, alignant ainsi les objectifs de mod√©lisation avec les imp√©ratifs business.</p>
                    
                    <h4>Sensibilit√© aux faibles volumes</h4>
                    <p>Pour les produits √† tr√®s faible rotation (ventes proches de z√©ro), le NWRMSLE peut devenir instable. Des strat√©gies de traitement sp√©cifiques (seuils minimaux, agr√©gation temporelle) sont souvent n√©cessaires pour √©quilibrer pr√©cision et stabilit√©.</p>
                </div>
                
                <h3>4.2 MAE (Mean Absolute Error)</h3>
                <div class="model-box">
                    <h4>Formule</h4>
                    <div class="formula">
                        MAE = (1/n) √ó Œ£ |y<sub>i</sub> - ≈∑<sub>i</sub>|
                    </div>
                    
                    <h4>Avantages en termes d'interpr√©tation m√©tier</h4>
                    <p>Le MAE est directement interpr√©table dans l'unit√© de mesure originale (unit√©s vendues). Une MAE de 15 signifie qu'en moyenne, les pr√©visions s'√©cartent de 15 unit√©s des ventes r√©elles. Cette transparence facilite la communication avec les √©quipes op√©rationnelles non techniques.</p>
                    
                    <p>Contrairement au RMSE (Root Mean Squared Error), le MAE est moins sensible aux outliers car il ne quadratique pas les erreurs. Dans le retail o√π des pics de demande sporadiques sont fr√©quents (promotions exceptionnelles, √©v√©nements locaux), cette robustesse est appr√©ciable.</p>
                    
                    <h4><span class="limitation">Limites dans le cas de s√©ries h√©t√©rog√®nes</span></h4>
                    <p>Le MAE traite toutes les erreurs de mani√®re √©gale, ind√©pendamment du volume de ventes. Une erreur de 50 unit√©s impacte identiquement le MAE qu'elle concerne un produit vendant 100 unit√©s/jour ou 10 000 unit√©s/jour. Cette indiff√©rence √† l'√©chelle rend le MAE peu adapt√© √† l'√©valuation globale sur un catalogue h√©t√©rog√®ne.</p>
                    
                    <p>Pour pallier cette limitation, on peut calculer des MAE par segments (cat√©gories produits, magasins) ou utiliser le MAPE (Mean Absolute Percentage Error), bien que ce dernier souffre de probl√®mes de division par z√©ro pour les faibles volumes.</p>
                </div>
                
                <h3>4.3 Temps de Calcul</h3>
                <div class="model-box">
                    <h4>Importance en production</h4>
                    <p>Le temps de calcul ne se limite pas √† l'entra√Ænement : il englobe √©galement l'inf√©rence (g√©n√©ration de pr√©visions), le feature engineering, et les op√©rations de maintenance du mod√®le. En production retail, les pr√©visions doivent souvent √™tre actualis√©es quotidiennement pour des milliers de combinaisons produit-magasin.</p>
                    
                    <p>Un mod√®le n√©cessitant 10 heures d'entra√Ænement quotidien peut bloquer les fen√™tres de maintenance et retarder la mise √† disposition des pr√©visions aux √©quipes op√©rationnelles. L'infrastructure technique (serveurs, co√ªts cloud) et l'agilit√© business (capacit√© √† r√©entra√Æner rapidement suite √† un changement) d√©pendent directement de cette m√©trique.</p>
                    
                    <div class="key-point">
                        <h4>Arbitrage performance pr√©dictive vs co√ªt computationnel</h4>
                        <p>Un mod√®le complexe am√©liorant le NWRMSLE de 0.5% mais n√©cessitant 20 fois plus de temps de calcul n'est g√©n√©ralement pas justifi√© √©conomiquement. L'analyse co√ªt-b√©n√©fice doit int√©grer :</p>
                        <p>‚Ä¢ Le gain business marginal d'une meilleure pr√©vision<br>
                        ‚Ä¢ Les co√ªts d'infrastructure suppl√©mentaires<br>
                        ‚Ä¢ La complexit√© de maintenance et de debugging<br>
                        ‚Ä¢ Les risques op√©rationnels (latence, d√©faillances)</p>
                    </div>
                    
                    <p>La courbe de Pareto performance/temps guide typiquement vers des mod√®les offrant 80-90% de la performance maximale th√©orique avec une fraction du co√ªt computationnel.</p>
                </div>
            </section>
            
            <section id="bonnes-pratiques">
                <h2>5. Bonnes Pratiques M√©thodologiques</h2>
                
                <h3>5.1 Importance du Baseline</h3>
                <div class="model-box">
                    <p>D√©buter syst√©matiquement par un baseline simple est une discipline fondamentale en data science. Le baseline remplit trois fonctions critiques :</p>
                    
                    <p><strong>Ancrage de r√©f√©rence</strong> : Il quantifie la difficult√© intrins√®que du probl√®me et le niveau de performance "gratuit" accessible sans mod√©lisation complexe.</p>
                    
                    <p><strong>Validation de la valeur ajout√©e</strong> : Si un mod√®le sophistiqu√© ne surpasse pas significativement le baseline, cela indique soit un probl√®me de mod√©lisation, soit l'absence de signal exploitable dans les donn√©es.</p>
                    
                    <p><strong>Solution de secours</strong> : En cas de d√©faillance du syst√®me ML en production, le baseline peut servir de fallback garantissant une continuit√© de service minimale.</p>
                </div>
                
                <h3>5.2 Validation Temporelle</h3>
                <div class="model-box">
                    <p>Les s√©ries temporelles violent l'hypoth√®se i.i.d. (ind√©pendante et identiquement distribu√©e) sous-jacente √† la validation crois√©e classique. Une approche time-series split doit √™tre adopt√©e :</p>
                    
                    <div class="formula">
                        Train: [t‚ÇÅ, t‚ÇÇ, ..., t‚Çô] ‚Üí Validation: [t‚Çô‚Çä‚ÇÅ, ..., t‚Çô‚Çä‚Çñ] ‚Üí Test: [t‚Çô‚Çä‚Çñ‚Çä‚ÇÅ, ..., t‚Çô‚Çä‚Çñ‚Çä‚Çò]
                    </div>
                    
                    <p>Cette s√©paration strictement temporelle simule la r√©alit√© op√©rationnelle : le mod√®le est entra√Æn√© sur le pass√© et doit pr√©dire le futur. Elle pr√©vient le data leakage temporel et fournit une estimation r√©aliste de la performance en production.</p>
                    
                    <p>Pour augmenter la robustesse de l'√©valuation, une validation crois√©e temporelle avec fen√™tres glissantes (rolling origin) peut √™tre impl√©ment√©e, testant le mod√®le sur plusieurs p√©riodes futures successives.</p>
                </div>
                
                <h3>5.3 Risques de Data Leakage</h3>
                <div class="model-box">
                    <p>Le data leakage, particuli√®rement insidieux dans les s√©ries temporelles, peut cr√©er une illusion de performance √©lev√©e qui s'effondre en production. Les sources principales sont :</p>
                    
                    <p><strong>Leakage temporel</strong> : Utilisation d'informations futures dans les features (ex : statistiques calcul√©es sur l'ensemble du dataset incluant les p√©riodes de test).</p>
                    
                    <p><strong>Leakage via la target</strong> : Features directement d√©riv√©es de la variable cible (ex : ratio calcul√© avec les ventes du jour √† pr√©dire).</p>
                    
                    <p><strong>Leakage via normalisation globale</strong> : Standardisation calcul√©e sur train+test puis appliqu√©e, incorporant des statistiques futures.</p>
                    
                    <p><strong>Pr√©vention</strong> : Toute transformation (imputation, normalisation, feature engineering) doit √™tre ajust√©e uniquement sur le set d'entra√Ænement puis appliqu√©e aux autres sets. Les pipelines scikit-learn garantissent cette discipline si correctement utilis√©s.</p>
                </div>
                
                <h3>5.4 S√©paration Entra√Ænement / Validation / Test</h3>
                <div class="model-box">
                    <p>La s√©paration tripartite des donn√©es est une protection fondamentale contre le sur-apprentissage et la s√©lection biais√©e de mod√®les :</p>
                    
                    <p><strong>Set d'entra√Ænement (60-70%)</strong> : Utilis√© pour l'ajustement des param√®tres du mod√®le (poids, coefficients).</p>
                    
                    <p><strong>Set de validation (15-20%)</strong> : Utilis√© pour le tuning des hyperparam√®tres et la s√©lection entre mod√®les concurrents. Ce set guide les d√©cisions m√©thodologiques.</p>
                    
                    <p><strong>Set de test (15-20%)</strong> : Utilis√© une seule fois, apr√®s finalisation compl√®te du mod√®le, pour estimer la performance r√©elle en conditions op√©rationnelles. Ce set ne doit JAMAIS influencer les choix de mod√©lisation.</p>
                    
                    <p>La contamination du test (utilisation r√©p√©t√©e pour comparer des variantes) transforme de facto ce set en validation suppl√©mentaire, biaisant √† la hausse l'estimation de performance. La discipline de n'√©valuer sur le test qu'une fois le mod√®le totalement fig√© est absolument critique pour la cr√©dibilit√© scientifique du projet.</p>
                </div>
            </section>
            
            <section id="conclusion">
                <h2>Conclusion</h2>
                
                <p>Ce projet de pr√©vision des ventes retail, inspir√© de la comp√©tition Kaggle Favorita, illustre une d√©marche m√©thodologique rigoureuse de s√©lection et d'√©valuation de mod√®les de machine learning. La comparaison syst√©matique de multiples approches, du baseline simple (moyenne mobile) aux techniques de gradient boosting avanc√©es (LightGBM), a permis d'identifier le mod√®le optimal selon un ensemble de crit√®res techniques et business.</p>
                
                <div class="highlight">
                    <h3>Enseignements Cl√©s</h3>
                    
                    <p><strong>1. La sup√©riorit√© du gradient boosting</strong> sur les donn√©es tabulaires retail est confirm√©e, avec LightGBM obtenant les meilleures performances (NWRMSLE: 0.398, MAE: 7.82) tout en restant computationnellement efficient (156s d'entra√Ænement).</p>
                    
                    <p><strong>2. L'importance du compromis performance-complexit√©</strong> : Un mod√®le n'est pertinent pour la production que s'il offre un gain pr√©dictif significatif justifiant son co√ªt additionnel en ressources et en maintenance.</p>
                    
                    <p><strong>3. La n√©cessit√© d'une √©valuation multicrit√®re</strong> : La d√©cision ne peut reposer uniquement sur une m√©trique pr√©dictive mais doit int√©grer temps de calcul, interpr√©tabilit√©, robustesse, et alignement avec les contraintes op√©rationnelles.</p>
                    
                    <p><strong>4. L'indispensabilit√© des bonnes pratiques</strong> : Baseline rigoureux, validation temporelle stricte, pr√©vention du leakage, et s√©paration train/validation/test constituent les fondations d'un projet ML cr√©dible et reproductible.</p>
                </div>
                
                <h3>Ouvertures et Perspectives</h3>
                
                <p><strong>Feature Engineering Avanc√©</strong> : L'am√©lioration des performances passe souvent davantage par l'ing√©nierie de features que par le changement de mod√®le. Des pistes incluent : agr√©gations temporelles multi-√©chelles, encodages de cyclicit√© (Fourier features), interactions explicites entre cat√©gories, embedding de produits similaires, et incorporation de donn√©es externes (m√©t√©o, √©v√©nements locaux, concurrence).</p>
                
                <p><strong>Mod√®les Hybrides</strong> : L'approche ensembliste combinant les pr√©dictions de plusieurs mod√®les compl√©mentaires (stacking, blending) peut exploiter les forces respectives des diff√©rentes familles. Par exemple, un ensemble LightGBM + CatBoost + Neural Network peut capturer des patterns que chaque mod√®le individuellement manquerait.</p>
                
                <p><strong>Deep Learning pour le Forecasting</strong> : Les architectures r√©centes (Transformers temporels, N-BEATS, DeepAR) montrent des r√©sultats prometteurs sur certains probl√®mes de s√©ries temporelles complexes. Bien que gourmandes en donn√©es et en calcul, elles m√©ritent exploration pour des datasets de tr√®s grande √©chelle ou lorsque des patterns complexes de long terme sont suspect√©s.</p>
                
                <p><strong>Pr√©vision Probabiliste</strong> : Au-del√† des pr√©visions ponctuelles, les mod√®les fournissant des intervalles de confiance ou des distributions compl√®tes (quantile regression, mod√®les bay√©siens) apportent une information pr√©cieuse pour la gestion du risque et l'optimisation sous incertitude.</p>
                
                <p><strong>AutoML et Optimisation d'Hyperparam√®tres</strong> : Les frameworks d'AutoML (Optuna, Ray Tune, AutoGluon) peuvent automatiser et am√©liorer le tuning des hyperparam√®tres, lib√©rant du temps data scientist pour des t√¢ches √† plus forte valeur ajout√©e.</p>
                
                <p>La pr√©vision des ventes demeure un domaine de recherche et d'innovation actif, o√π les avanc√©es m√©thodologiques se traduisent directement en impact business mesurable. L'adoption de LightGBM comme standard industriel refl√®te l'√©tat de l'art actuel, mais l'√©volution continue des techniques et des outils promet des am√©liorations futures substantielles.</p>
            </section>
            
            <section id="references" class="references">
                <h2>R√©f√©rences</h2>
                
                <ol>
                    <li><strong>LightGBM Documentation Officielle</strong><br>
                    Microsoft Research. "LightGBM: A Highly Efficient Gradient Boosting Decision Tree."<br>
                    <em>https://lightgbm.readthedocs.io/</em></li>
                    
                    <li><strong>Ke, G., Meng, Q., Finley, T., et al. (2017)</strong><br>
                    "LightGBM: A Highly Efficient Gradient Boosting Decision Tree."<br>
                    <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em></li>
                    
                    <li><strong>Kaggle Competition: Corporaci√≥n Favorita Grocery Sales Forecasting</strong><br>
                    "Can you accurately predict sales for a large grocery chain?"<br>
                    <em>https://www.kaggle.com/c/favorita-grocery-sales-forecasting</em></li>
                    
                    <li><strong>Chen, T., & Guestrin, C. (2016)</strong><br>
                    "XGBoost: A Scalable Tree Boosting System."<br>
                    <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em></li>
                    
                    <li><strong>Prokhorenkova, L., Gusev, G., Vorobev, A., et al. (2018)</strong><br>
                    "CatBoost: unbiased boosting with categorical features."<br>
                    <em>Advances in Neural Information Processing Systems 31 (NeurIPS 2018)</em></li>
                    
                    <li><strong>Hastie, T., Tibshirani, R., & Friedman, J. (2009)</strong><br>
                    "The Elements of Statistical Learning: Data Mining, Inference, and Prediction."<br>
                    <em>Springer Series in Statistics, 2nd Edition</em></li>
                    
                    <li><strong>Hyndman, R.J., & Athanasopoulos, G. (2021)</strong><br>
                    "Forecasting: Principles and Practice."<br>
                    <em>OTexts: Melbourne, Australia. 3rd Edition. https://otexts.com/fpp3/</em></li>
                    
                    <li><strong>Brownlee, J. (2020)</strong><br>
                    "How to Develop a Light Gradient Boosting Machine for Time Series Forecasting."<br>
                    <em>Machine Learning Mastery. https://machinelearningmastery.com/</em></li>
                    
                    <li><strong>Kaggle Winning Solutions</strong><br>
                    "1st Place Solution - Corporaci√≥n Favorita Grocery Sales Forecasting."<br>
                    <em>Kaggle Discussion Forums</em></li>
                    
                    <li><strong>Tofallis, C. (2015)</strong><br>
                    "A better measure of relative prediction accuracy for model selection and model estimation."<br>
                    <em>Journal of the Operational Research Society, 66(8), 1352-1362</em></li>
                    
                    <li><strong>Breiman, L. (2001)</strong><br>
                    "Random Forests."<br>
                    <em>Machine Learning, 45(1), 5-32</em></li>
                    
                    <li><strong>Tibshirani, R. (1996)</strong><br>
                    "Regression Shrinkage and Selection via the Lasso."<br>
                    <em>Journal of the Royal Statistical Society Series B, 58(1), 267-288</em></li>
                    
                    <li><strong>Lundberg, S.M., & Lee, S.I. (2017)</strong><br>
                    "A Unified Approach to Interpreting Model Predictions."<br>
                    <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em></li>
                    
                    <li><strong>Makridakis, S., Spiliotis, E., & Assimakopoulos, V. (2020)</strong><br>
                    "The M4 Competition: 100,000 time series and 61 forecasting methods."<br>
                    <em>International Journal of Forecasting, 36(1), 54-74</em></li>
                </ol>
            </section>
        </div>
        
        <footer>
            <p><strong>Note Technique ‚Äì Sales Forecasting</strong></p>
            <p>Document acad√©mique et professionnel ‚Ä¢ Corporaci√≥n Favorita Kaggle Competition</p>
            <p>¬© 2024 - Tous droits r√©serv√©s</p>
        </footer>
    </div>
</body>
</html>